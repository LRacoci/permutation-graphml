\begin{abstract}
To test Deepmind's Graph Nets framework against invariance with respect to permutations in graphs representation, we used three different problems in at least four different contexts: with permuted and non-permuted training sets; and permuted and non-permuted testing sets.

In all these contexts, the common behavior was that permuting testing sets reduced the accuracy of the networks and permuting training sets produced better results. This supports the argument that the algorithms might not be learning from the graph structure itself, but from the graph's computational representation.
\end{abstract}
