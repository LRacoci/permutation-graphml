\begin{abstract}
To test Deepmind's Graph Nets framework against invariance with respect to permutations in the graphs representation, we used three different problems in at least four different scenarios each: with and without permutations in the representation of entries of train and test sets.

In all those scenarios, the common behavior was that permuting test sets affected way less the results than permuting training. These results reinforce the argument that the algorithms might not be learning from the graph structure itself, but from the graph computational representation.
\end{abstract}
