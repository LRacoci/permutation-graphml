\section{Introduction}

\subsection{Overview}
Artificial Intelligence has gone through a renaissance with progress on computer vision, natural language processing, automation and control and decision problems ~\cite{Battaglia_2018}. Since 1986 ~\cite{Rumelhart_1986} and mainly after 2006 ~\cite{Hinton_2006}, Artificial Neural Networks has begun to stand out with Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) (Fig.\ref{fig:simple_neural_network}).


\begin{figure}[H]
\centering
\subfigure[Fully connected]{
    \includegraphics[width=.3\textwidth]{fig/content/intro/simple_neural_network/fully_connected.pdf}
}
\subfigure[Convolutional]{
    \includegraphics[width=.3\textwidth]{fig/content/intro/simple_neural_network/convolutional.pdf}
}
\subfigure[Recurrent]{
    \includegraphics[width=.3\textwidth]{fig/content/intro/simple_neural_network/recurrent.pdf}
}
\caption{Simplified representation of neural networks models ~\cite{Battaglia_2018}}

\label{fig:simple_neural_network}
\end{figure}

Between 2017 and 2018, papers ~\cite{Battaglia_2018, Gilmer_2017, Wang_2018} describing generalized graph prediction algorithms were published. These would provide a framework for predicting, with some acceptable error, output graphs given input graphs. Applications of these solutions varies from modelling molecular structures in Chemistry to Corpuscular Mechanics, and Community Studies.

In 2019, January 2nd a broad review ~\cite{Zhou_2019} of the main graph networks algorithms was carried out confirming that the Graph Nets’ approach ~\cite{Battaglia_2018} generalizes the Message Passing Neural Network ~\cite{Gilmer_2017} and Non Local Neural Network ~\cite{Wang_2018} methods, which in turn generalize other approaches. Some of those methods will be further detailed in the next section.


\subsection{Invariant Networks}

Given a graph $G = (V, A)$ consisting of $n$ nodes $V$ and values $A$ attached to its edges, a functional relation $f(A^l) \approx T^l$ (where $f$ is a neural network and $T^l$ are the corresponding targets), is order invariant if it produces the same output regardless of the node numbering used to encode $A$. For example, given an arbitrary adjacency matrix $A = A \in \mathds{R}^{n \times n}$, representing a graph, and an arbitrary permutation matrix $P$, the function $f$ is order invariant if it satisfies $f(P^T A P) = f(A)$.

Since the neighborhood aggregation approach is as powerful as the Weisfeiler-Lehman graph isomorphism test ~\cite{Xu_2018}, we conjectured that the the Graph Nets’ approach would not be order invariant, which would result in limitations on the algorithms’ performance when dealing with permuted graphs. 